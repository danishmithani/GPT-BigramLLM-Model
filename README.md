This Repository hold code for you to train a GPT modela nd Bigram model in respective folder. 

Dataset used: wizard_of_oz.txt  - for Bigram model
              openwebtext - for GPT model (dataset available on google in the form of a drive link)

OpenWebText.py helps data preprocessing, helping to create vocabulory and single large trainable file + validation file.
(dataset Not included due to size constraints.)