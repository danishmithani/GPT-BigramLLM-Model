This Repository holds the code for you to train a GPT model and Bigram model in respective folder. 

Reference (with good enough explaination) : 
    Link One: https://www.youtube.com/watch?v=PaCmpygFfXo
    Link two: https://www.youtube.com/watch?v=UU1WVnMk4E8&t=19590s
    
Dataset used: wizard_of_oz.txt  - for Bigram model
              openwebtext - for GPT model (dataset available on google in the form of a drive link)

OpenWebText.py helps data preprocessing, helping to create vocabulory and single large trainable file + validation file.
(dataset Not included due to size constraints.)

The code is flooded with comments from across different sources for ease of understanding of each line and its essence.
