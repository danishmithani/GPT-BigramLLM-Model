{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFile built on top of BigramModel file which contains \\ntransforer code and explaination.\\n\\nAbove was Transformer but we will look into GPT (Generative Pre-trained Trasformer). where only Decoder part stay.. and not the Encoder nor the 2nd unmasked Multihead attention (refer research paper image)\\nBefore this, lets look into Self Attention\\n\\nDecoder only structure (GPT Structure)\\n\\nTraditional Inputs ---> Embeddings+positional encoding ---> decoders X N one after another (sequential)--->nnLinear --->softmax --->samplea dn generating output and compare to imput to optimize\\nWithin each decoder: We have :\\nMultihead attention ---> residual connection+norm --->feed forward ---> residual connect+norm\\n\\nMultihead Attention part in Decoder:\\n-Improtant: imagine different people(heads) learning same thing from different perspective.\\n-all heads have different learnable parameters.\\n-Keys (K), Query(Q) and Value(V) in each head, togethere go into Scaled Dot product attention ---> concat results ---> Linear Layer\\n1. K, Q and V are essense of self attention models. (explained later)\\n2. Scaled dot Prod. Attention: there are N such heads with respective K, Q and Vs which work and give N outputs\\n3. Hence we need concat results section which then is passed to nn.linear \\n\\nScaled Dot Prod Attention Part in Multihead Attention:\\n-Key(K) emits different tensor for every token in sentence\\n-Query(Q) says, what am i looking for ? for some important words, dot product of K and Q will be very high, unlike some other words like \"A, an, The etc.\"\\n1. we now scale this dot prod by 1/ sqrt (len of row in keys) helps prevent dot products explode.\\n2. This scaled value is now fed to torch.tril() which helps learn from the tokens we have already seen while keeping ones ahead = 0 (look at Tril matrix, ull understand..). This process is called Attention Masking.\\n3. we then apply softmax to above output. why? because it will then give important to the few attentions that matter more as comapred to other. This helps increase confidence of the model.\\n4. matrix multipy it with original Value(V) to retain info if lost in computation.\\n\\n\\nImportant:\\nKey points to note on how transformer is different from GPT below.\\n1. Transformers have embeddings as we have seen (nn.embedding) which has its dimentions for every token in consideration. there is although no way to transmit \"positional importance\" wrt previous and next tokens.\\n2. Positional encodding is a method to push some positional patterns so that model can consider them in its calculations.\\n   - This is done by creating a positional encoding (PE) vector for every dimension a token has. in our case, dim. = vocab_size.\\n   \\n   eg. for 5th character, having 87 dimensional embedding vector now have PE:\\n   PE(5, 0=dim) = sin(pos/(10000^(0/87)))\\n   PE(5, 1=dim) = cos(pos/(10000^(1/87)))\\n   PE(5, 2=dim) = sin(pos/(10000^(2/87)))\\n   PE(5, 3=dim) = cos(pos/(10000^(3/87)))   .. and so on till  87.\\n   Note the alternating Sine and cosine.\\n   Even Dimensions (0, 2, 4, ...): These dimensions undergo a sine transformation. For a given token, every even dimension will have its value computed using the sine function.\\n   Odd Dimensions (1, 3, 5, ...): These dimensions undergo a cosine transformation. For a given token, every odd dimension will have its value computed using the cosine function.\\n\\n3. This way, eg. PE(5, 0=dim) has not built a mathematical relation with PE(4, 0=dim) and PE(6, 0=dim) so on for each dimension.\\n4. PE is restricted to number of tokens considered in a block. For us, we have considered 8, so math tries to find relations only within these 8 tokens in one block.\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "File built on top of BigramModel file which contains \n",
    "transforer code and explaination.\n",
    "\n",
    "Above was Transformer but we will look into GPT (Generative Pre-trained Trasformer). where only Decoder part stay.. and not the Encoder nor the 2nd unmasked Multihead attention (refer research paper image)\n",
    "Before this, lets look into Self Attention\n",
    "\n",
    "Decoder only structure (GPT Structure)\n",
    "\n",
    "Traditional Inputs ---> Embeddings+positional encoding ---> decoders X N one after another (sequential)--->nnLinear --->softmax --->samplea dn generating output and compare to imput to optimize\n",
    "Within each decoder: We have :\n",
    "Multihead attention ---> residual connection+norm --->feed forward ---> residual connect+norm\n",
    "\n",
    "Multihead Attention part in Decoder:\n",
    "-Improtant: imagine different people(heads) learning same thing from different perspective.\n",
    "-all heads have different learnable parameters.\n",
    "-Keys (K), Query(Q) and Value(V) in each head, togethere go into Scaled Dot product attention ---> concat results ---> Linear Layer\n",
    "1. K, Q and V are essense of self attention models. (explained later)\n",
    "2. Scaled dot Prod. Attention: there are N such heads with respective K, Q and Vs which work and give N outputs\n",
    "3. Hence we need concat results section which then is passed to nn.linear \n",
    "\n",
    "Scaled Dot Prod Attention Part in Multihead Attention:\n",
    "-Key(K) emits different tensor for every token in sentence\n",
    "-Query(Q) says, what am i looking for ? for some important words, dot product of K and Q will be very high, unlike some other words like \"A, an, The etc.\"\n",
    "1. we now scale this dot prod by 1/ sqrt (len of row in keys) helps prevent dot products explode.\n",
    "2. This scaled value is now fed to torch.tril() which helps learn from the tokens we have already seen while keeping ones ahead = 0 (look at Tril matrix, ull understand..). This process is called Attention Masking.\n",
    "3. we then apply softmax to above output. why? because it will then give important to the few attentions that matter more as comapred to other. This helps increase confidence of the model.\n",
    "4. matrix multipy it with original Value(V) to retain info if lost in computation.\n",
    "\n",
    "\n",
    "Important:\n",
    "Key points to note on how transformer is different from GPT below.\n",
    "1. Transformers have embeddings as we have seen (nn.embedding) which has its dimentions for every token in consideration. there is although no way to transmit \"positional importance\" wrt previous and next tokens.\n",
    "2. Positional encodding is a method to push some positional patterns so that model can consider them in its calculations.\n",
    "   - This is done by creating a positional encoding (PE) vector for every dimension a token has. in our case, dim. = vocab_size.\n",
    "   \n",
    "   eg. for 5th character, having 87 dimensional embedding vector now have PE:\n",
    "   PE(5, 0=dim) = sin(pos/(10000^(0/87)))\n",
    "   PE(5, 1=dim) = cos(pos/(10000^(1/87)))\n",
    "   PE(5, 2=dim) = sin(pos/(10000^(2/87)))\n",
    "   PE(5, 3=dim) = cos(pos/(10000^(3/87)))   .. and so on till  87.\n",
    "   Note the alternating Sine and cosine.\n",
    "   Even Dimensions (0, 2, 4, ...): These dimensions undergo a sine transformation. For a given token, every even dimension will have its value computed using the sine function.\n",
    "   Odd Dimensions (1, 3, 5, ...): These dimensions undergo a cosine transformation. For a given token, every odd dimension will have its value computed using the cosine function.\n",
    "\n",
    "3. This way, eg. PE(5, 0=dim) has not built a mathematical relation with PE(4, 0=dim) and PE(6, 0=dim) so on for each dimension.\n",
    "4. PE is restricted to number of tokens considered in a block. For us, we have considered 8, so math tries to find relations only within these 8 tokens in one block.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "block_size = 32\n",
    "batch_size = 128       #Each batch has 4 blocks under them\n",
    "max_iters = 200\n",
    "eval_iters = 100       #To print loss updates\n",
    "lr = 3e-4              #very common ones to try are:3e-3, 1e-3, 3e-4, 1e-3\n",
    "n_head = 1\n",
    "n_embd = 384         #similar to waht we used in BigramModel(vocab_size) ie. Vector size. \n",
    "X_layers = 1         #No of Sequential decoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('openwebtext/vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# creating vocabulary\n",
    "chars = sorted(set(text))  # len = 81\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# now we tokenize them - basically giving them an incremental integer numbers to address them\n",
    "str_to_int = { ch:i for i, ch in enumerate(chars) }\n",
    "int_to_str = { i:ch for i, ch in enumerate(chars) }\n",
    "encode = lambda s: [str_to_int[c] for c in s]          #encode(\"Hello\")\n",
    "decode = lambda I: ''.join([int_to_str[i] for i in I]) #decode([12, 45, 2, 78])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Output/output_train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n\u001b[1;32m---> 31\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m, in \u001b[0;36mget_batch\u001b[1;34m(split)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_batch\u001b[39m(split):\n\u001b[1;32m---> 24\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mget_random_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;241m-\u001b[39mblock_size, (batch_size,))\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i:t\u001b[38;5;241m+\u001b[39mblock_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m, in \u001b[0;36mget_random_chunk\u001b[1;34m(split)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_random_chunk\u001b[39m(split):\n\u001b[0;32m      8\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput/output_train.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput/output_val.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:      \u001b[38;5;66;03m#as the file is 37.42GB, it is too long for a simple open() func to carry. we need to use memory Map reading\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m mmap\u001b[38;5;241m.\u001b[39mmmap(f\u001b[38;5;241m.\u001b[39mfileno(), \u001b[38;5;241m0\u001b[39m, access \u001b[38;5;241m=\u001b[39m mmap\u001b[38;5;241m.\u001b[39mACCESS_READ) \u001b[38;5;28;01mas\u001b[39;00m mappedFile:  \u001b[38;5;66;03m#mmap helps bring file straight to RAM sacrificing sapce for time to help you read chunk without opening whole file. f.fileno(): RAM numbers eevry open file. this command helps link mmap func to that file. 0: indicating that the system should create a memory map large enough to hold the entire file. access for mmap should match access fiven to open()\u001b[39;00m\n\u001b[0;32m     11\u001b[0m             size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(mappedFile)\n",
      "File \u001b[1;32mD:\\fcc-llm-course\\cuda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Output/output_train.txt'"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "valid_data = data[n:]\n",
    "\n",
    "def get_random_chunk(split):\n",
    "    filename = \"Output/output_train.txt\" if split == \"train\" else \"Output/output_val.txt\"\n",
    "    with open(filename, 'rb') as f:      #as the file is 37.42GB, it is too long for a simple open() func to carry. we need to use memory Map reading\n",
    "        with mmap.mmap(f.fileno(), 0, access = mmap.ACCESS_READ) as mappedFile:  #mmap helps bring file straight to RAM sacrificing sapce for time to help you read chunk without opening whole file. f.fileno(): RAM numbers eevry open file. this command helps link mmap func to that file. 0: indicating that the system should create a memory map large enough to hold the entire file. access for mmap should match access fiven to open()\n",
    "            size = len(mappedFile)\n",
    "            start_pos = random.randint(0, size-block_size*batch_size)\n",
    "            #go to start postion now\n",
    "            mappedFile.seek(start_pos)\n",
    "            chunk = mappedFile.read(block_size*batch_size-1)  # -1 is precaution to avoid overflow\n",
    "            \n",
    "            #block we got is encoded. we need to decode it\n",
    "            word_block = chunk.decode('utf-8', errors='ignore').replace('\\r', '')    #errors=ignored doesnt stop execution, just ignored the error pathc and considers next one to decode.\n",
    "            int_data = torch.tensor(encode(word_block), dtype=torch.long)            #we are interested only in intergers associated with each token word/character\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:t+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:t+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()         #temporarily disable gradient computation when the below function is called.\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()        # helps diable operations specific to training phase \n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)   # model calculating loss N (we have arbitrarily kept N=eval_iters but it can be any large number to make sure loss is avg over several samples) times and then avg. it out \n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()       # model is set back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)      \n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))      #registers this state of no-look-ahead in a form of a model. helps save computation as we dont need to redo this step later.\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input of size (B, T, C)\n",
    "        # Output of  (B, T, Head_size)\n",
    "        \n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   #(B, T, Head_size)\n",
    "        q = self.query(x) #(B, T, Head_size)\n",
    "        \n",
    "        attn_scores = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5   # (B, T, Head_size) @ (B, Head_size, T) --> (B, T, T)   #transpose(-2,-1) is swapping 2ndlast dim to last dim. the whole line helps moderate values coming out of all heads and no single head is dominant. hence all the voices from all heads is scalled down by sqrt them.\n",
    "        attn_scores = attn_scores.masked_fill(self.tril[:T, :T] ==0, float('-inf'))    #(B, T, T)   #upper side of triangle in matrix is -inf. Softmax next turns it to 0\n",
    "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
    "        attn_scores = self.dropout(attn_scores)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = attn_scores @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):   # multiple heads in parallel\n",
    "    def __init__(self, n_head, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])     # container class provided by PyTorch to hold a list of PyTorch modules. helps run things in parallel unlike nn.sequential\n",
    "        self.proj = nn.Linear(head_size * n_head, n_embd)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.concat([h(x) for h in self.heads], dim=-1)   # (B, T, F) becomes (B, T, [h1 h1 h1, h2, h2, h2, h3, h3, h3, h3]) where H are features concat. accross last dim.\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(0.2)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head   #num. of features each head is capturing\n",
    "        self.selfAtten = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)  # for 2 postNorm architectures.\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.selfAtten(x)\n",
    "        x = self.ln1(x+y)    #we are first adding x and y then applying a Norm on it\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x+y)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLangModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd):\n",
    "        super().__init__()                   #The super().__init__() call ensures that the BigramLangModel class correctly inherits the behavior and properties of the nn.Module class. This practice allows you to build complex neural network models while benefiting from the foundation provided by PyTorch's nn.Module.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)     #here, first dim is vocab. of unique tokens (vocab_length) and 2nd dimention is the length of the dense vector for that index token (embedding dimension)\n",
    "        #The embedding dimension is a crucial parameter because it determines the capacity of the embedding space to capture and represent information about the tokens. Larger embedding dimensions can potentially capture more complex patterns and relationships in the data but may also require more computational resources.\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        #Creating X_layers of sequential (Decoder layer blocks)\n",
    "        self.blocks = nn.Sequential(*[DecoderBlock(n_embd, n_head=n_head) for _ in range(X_layers)])   # * helps create X No of detatched nn.sequent. blocks \n",
    "        self.ln_f = nn.LayerNorm(n_embd)  #creating a checkpoint for the final layer after laster decoder, ie. a Linear Layer. It is not a layer but a technique to normalize values coming from decoder before feeding it into Linear Layer. It applies normalization to the veatures of teach token in the batch and has its own learnable parameters. this step will be removed later (maybe)\n",
    "        self.ln_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)  #method by nn.Module class to apply weights to all sub-classes and self. apply takes care of which module to pass as parameter hence we dont.\n",
    "    \n",
    "    def _init_weights(self, module):    #Private method to init.weights. module is any sub-module(layer) of the nn\n",
    "        if isinstance(module, nn.Linear):  #checks if module is an instance of Linear or Embedding\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)  #init. weights using normal distr.\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)    #Initializing biases to zeros is a common practice, especially when weights are initialized with small random values.\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        \n",
    "        tok_embed = self.token_embedding_table(index)                                  # (B, T=block_size, C) similar to logits previously\n",
    "        pos_embed = self.positional_embedding_table(torch.arange(T, device=device))   # (T, C). performs an embedding lookup. It takes the indices generated by torch.arange and retrieves the corresponding rows from the embedding table.\n",
    "        # Good practice to add both above embeddings\n",
    "        X = tok_embed + pos_embed   # becomes (B, T, C) refer \"Broadcasting Sementics pytorch\" to know how\n",
    "        X = self.blocks(X)          # (B, T, C)\n",
    "        X = self.ln_f(X)            # (B, T, C)\n",
    "        logits = self.ln_head(X)    # (B, T, vocab_size)\n",
    "        \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape         #B = Batch, T = Time(unknown/next seq. of integers), C = channels/vocab_size\n",
    "            logits = logits.view(B*T, C)   # .view helps repack into a new shape as per need. we need 2nd parameter to be C, doesnt matter what first is.\n",
    "            targets = targets.view(B*T)     #helped reduce dimension.\n",
    "            loss = F.cross_entropy(logits, targets)    # above operation on logits was done as cross_entropy accepts 1st input with shape (X, Channels) and not (X, Y, channels). Targets simmilary should be of shape (X*Y),  i.e only 1 dim\n",
    "            # Cross-entropy is a measure of the difference between two probability distributions for a given random variable. In the context of machine learning, and particularly in classification tasks, cross-entropy is commonly used as a loss function to measure the difference between the predicted probability distribution and the true probability distribution of the target\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            #predictions\n",
    "            logits, loss = self.forward(index) # no target given hence \"else loop\" in forward doesnt run. Loss = none and logit's shape is (dim of index(1,1), vocab_size(ie. 87))\n",
    "            \n",
    "            #Focussing only on last Time step and not all of T\n",
    "            logits = logits[:, -1, :] # to get (B, C). why only last T?\n",
    "            #softmax\n",
    "            probs = F.softmax(logits, dim=-1) #focussing only on last dimension of logits here. Output of model is passed to softmax to create probablities\n",
    "            \n",
    "            #sample from distribution\n",
    "            #----Consider the scenario where you are generating text, and at a certain point, the model has multiple plausible options for the next character. If you always choose the most probable character, the generated sequences might become deterministic and lack diversity. By introducing randomness through sampling, you allow the model to explore different paths and create more diverse outputs.\n",
    "            index_next = torch.multinomial(probs, num_samples=1) #(B, 1) eg. [54]\n",
    "            index = torch.cat((index, index_next), dim=1) #(B, T+1)\n",
    "        return index\n",
    "    \n",
    "model = GPTLangModel(vocab_size, n_embd)\n",
    "print('loading model...')\n",
    "with open('model_001.pkl', 'rb') as f:    #helps retrain model automatically\n",
    "    model = pickle.load(f)\n",
    "\n",
    "print('success...')\n",
    "m = model.to(device)\n",
    "\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "# print(generated_chars) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4016,  0.0422,  1.0896,  0.2567,  0.2861, -0.5000,  0.2231, -0.4832,\n",
      "         -0.3609, -0.1560]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([1, 10])\n",
      "tensor([[[-0.4016,  0.0422,  1.0896,  0.2567,  0.2861, -0.5000,  0.2231,\n",
      "          -0.4832, -0.3609, -0.1560]]], grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "#Dummy code to understand embedding\n",
    "# more info in: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "word_to_ind = {\"hello\":0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 10)\n",
    "\n",
    "\n",
    "lookup_tensor1 = torch.tensor([0], dtype=torch.long)\n",
    "# print(lookup_tensor.shape)\n",
    "lookup_tensor2 = torch.zeros((1, 1), dtype=torch.long)\n",
    "xyz = embeds(lookup_tensor1)\n",
    "zxy = embeds(lookup_tensor2)\n",
    "print(xyz)\n",
    "print(xyz.shape)\n",
    "print(zxy)\n",
    "print(zxy.shape)\n",
    "# Note: we passed a tensor of dim 3. hence output was (3, 10). if we had vocab size of 10,000 and had passed tensor of dim 65, we would have gottent tensor of dim (65, 10))\n",
    "#these outputs are called logits (not correct values though as they are not trained with training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_iters \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 6\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\fcc-llm-course\\cuda\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 9\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m      8\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[1;32m----> 9\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# model calculating loss N (we have arbitrarily kept N=eval_iters but it can be any large number to make sure loss is avg over several samples) times and then avg. it out \u001b[39;00m\n\u001b[0;32m     10\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     11\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mD:\\fcc-llm-course\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\fcc-llm-course\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[75], line 29\u001b[0m, in \u001b[0;36mGPTLangModel.forward\u001b[1;34m(self, index, targets)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Good practice to add both above embeddings\u001b[39;00m\n\u001b[0;32m     28\u001b[0m X \u001b[38;5;241m=\u001b[39m tok_embed \u001b[38;5;241m+\u001b[39m pos_embed   \u001b[38;5;66;03m# becomes (B, T, C) refer \"Broadcasting Sementics pytorch\" to know how\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(X)            \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_head(X)    \u001b[38;5;66;03m# (B, T, vocab_size)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\fcc-llm-course\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\fcc-llm-course\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\fcc-llm-course\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\fcc-llm-course\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\fcc-llm-course\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[74], line 11\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselfAtten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x\u001b[38;5;241m+\u001b[39my)    \u001b[38;5;66;03m#we are first adding x and y then applying a Norm on it\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(x)\n",
      "File \u001b[1;32mD:\\fcc-llm-course\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\fcc-llm-course\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[72], line 10\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m      9\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([h(x) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# (B, T, F) becomes (B, T, [h1 h1 h1, h2, h2, h2, h3, h3, h3, h3]) where H are features concat. accross last dim.\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mD:\\fcc-llm-course\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\fcc-llm-course\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\fcc-llm-course\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Optimizer code\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)   #AdamW applies Decay (penalty) to large weights. Adam Doesn't.\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'iteration: {iter}, Loss: {losses}')\n",
    "        \n",
    "    xb, yb = get_batch(\"train\")\n",
    "    \n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)    #previous grad. calculations are changed to 0. They are usually usefull in RNN model where reference to previous grad calculations is an important idea.\n",
    "    loss.backward()\n",
    "    optimizer.step()     #to take step in the right direction\n",
    "print(loss.item())\n",
    "\n",
    "with open('model_001.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)           #we could have used torch.load and torch.save as always but this is more straight forward and easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda-GPT",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
